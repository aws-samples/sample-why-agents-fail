{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Tool Discovery: Reducing Tool Selection Hallucinations\n",
    "\n",
    "Based on: [Internal Representations as Indicators of Hallucinations in Agent Tool Selection](https://arxiv.org/pdf/2601.05214)\n",
    "\n",
    "## The Problem\n",
    "\n",
    "When an AI agent has access to many tools with similar names and descriptions (e.g., `search_hotels`, `search_hotel_reviews`, `get_hotel_details`, `get_hotel_pricing`), it often picks the wrong one. The paper calls this **tool selection hallucination** ‚Äî the agent confidently chooses a tool that doesn't match the user's intent.\n",
    "\n",
    "This gets worse as the number of tools grows. With 5 tools, the agent rarely makes mistakes. With 30+, confusion between similar tools becomes a real problem.\n",
    "\n",
    "## The Technique: Semantic Pre-filtering with FAISS\n",
    "\n",
    "Instead of giving the agent all 31 tools at once, we use a two-step approach:\n",
    "\n",
    "**Step 1 ‚Äî Index tools by meaning**: We take each tool's name + docstring (e.g., `\"get_hotel_pricing: Get room pricing for a hotel.\"`) and convert it into a vector embedding using `SentenceTransformer('all-MiniLM-L6-v2')`. These embeddings are stored in a FAISS index for fast similarity search.\n",
    "\n",
    "**Step 2 ‚Äî Pre-filter before the agent sees them**: When a user query arrives, we embed it with the same model, search the FAISS index for the top-3 closest tools, and create the agent with **only those 3 tools**. The agent never sees the other 28.\n",
    "\n",
    "```\n",
    "User query ‚Üí Embed ‚Üí FAISS search ‚Üí Top-3 tools ‚Üí Agent(tools=top3)\n",
    "```\n",
    "\n",
    "## Why This Works\n",
    "\n",
    "- **91% fewer choices**: 3 tools instead of 31 means less room for confusion\n",
    "- **No generic traps**: Ambiguous tools like `search()` or `get_info()` are filtered out unless truly relevant\n",
    "- **Semantic, not keyword**: \"How much does Hotel Marriott cost?\" matches `get_hotel_pricing` by meaning, not just by the word \"hotel\"\n",
    "\n",
    "## What We Test\n",
    "\n",
    "We run 13 queries against both approaches and compare:\n",
    "- **Traditional**: Agent gets all 31 tools\n",
    "- **Semantic**: Agent gets only the top-3 tools per query\n",
    "\n",
    "Each query has a known correct tool (ground truth). We measure how often each approach picks the right one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install requirements.txt -r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import io\n",
    "import re\n",
    "\n",
    "from strands import Agent\n",
    "from enhanced_tools import ALL_TOOLS\n",
    "from registry import build_index, search_tools, get_scores\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(ALL_TOOLS)} tools\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Semantic Index\n",
    "\n",
    "For each tool, we concatenate `name: docstring` and encode it with `all-MiniLM-L6-v2` (384-dim vectors). FAISS stores these vectors and performs L2 nearest-neighbor search. The score shown is `1/(1+distance)` ‚Äî closer to 1.0 means better match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_index(ALL_TOOLS)\n",
    "\n",
    "# Test semantic search\n",
    "test_query = \"Find hotels in Spain\"\n",
    "print(f\"\\nüîç Query: '{test_query}'\")\n",
    "print(\"\\nTop 5 semantically similar tools:\")\n",
    "scores = get_scores(test_query, top_k=5)\n",
    "for i, s in enumerate(scores, 1):\n",
    "    print(f\"{i}. {s['name']:30} (score: {s['score']:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Queries (Ground Truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TESTS = [\n",
    "    # Original tests\n",
    "    (\"What's the weather in Paris?\", \"get_weather\"),\n",
    "    (\"Find flights from NYC to London\", \"search_flights\"),\n",
    "    (\"Book a hotel in Rome for John\", \"book_hotel\"),\n",
    "    (\"Check flight status for AA123\", \"get_flight_status\"),\n",
    "    (\"How much does Hotel Marriott cost?\", \"get_hotel_pricing\"),\n",
    "    (\"Cancel my reservation\", \"cancel\"),\n",
    "    (\"Search hotels in Barcelona\", \"search_hotels\"),\n",
    "    (\"Flight prices to Tokyo\", \"search_flight_prices\"),\n",
    "    \n",
    "    # Enhanced tests with real data\n",
    "    (\"Show me top-rated hotels\", \"get_top_hotels\"),\n",
    "    (\"Find hotels in France with rating above 9\", \"search_real_hotels\"),\n",
    "    (\"Convert 500 USD to EUR\", \"get_currency_exchange\"),\n",
    "    (\"Do I need a visa for Spain from USA?\", \"get_travel_documents\"),\n",
    "    (\"Check availability for Hilton on 2026-03-15 to 2026-03-18\", \"check_hotel_availability_dates\"),\n",
    "]\n",
    "\n",
    "print(f\"üìã Test suite: {len(TESTS)} queries\")\n",
    "print(f\"üìä Tool pool: {len(ALL_TOOLS)} tools\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_and_capture(agent, query):\n",
    "    \"\"\"Run agent and capture tool calls\"\"\"\n",
    "    old_stdout = sys.stdout\n",
    "    sys.stdout = io.StringIO()\n",
    "    \n",
    "    try:\n",
    "        result = agent(query)\n",
    "        output = sys.stdout.getvalue()\n",
    "    finally:\n",
    "        sys.stdout = old_stdout\n",
    "    \n",
    "    tools = re.findall(r'Tool #\\d+: (\\w+)', output)\n",
    "    return tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Test 1: Traditional Approach (All Tools)\n",
    "\n",
    "The agent receives all 31 tools in its system prompt. It must figure out which one to call based on the tool names and docstrings alone. With 7 generic tools (`search`, `check`, `get_details`, `get_status`, `get_info`, `book`, `cancel`) competing against specific ones, the model is likely to pick a generic or similar-but-wrong tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"us.anthropic.claude-3-haiku-20240307-v1:0\"\n",
    "PROMPT = \"You are a travel assistant. Use the correct tool to answer questions.\"\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(f\"TEST 1: TRADITIONAL - {len(ALL_TOOLS)} tools\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "trad_results = []\n",
    "trad_correct = 0\n",
    "\n",
    "for query, expected in TESTS:\n",
    "    agent = Agent(tools=ALL_TOOLS, system_prompt=PROMPT, model=MODEL)\n",
    "    tools = run_and_capture(agent, query)\n",
    "    ok = expected in tools\n",
    "    trad_correct += ok\n",
    "    trad_results.append({'query': query, 'expected': expected, 'actual': tools, 'correct': ok})\n",
    "    print(f\"{'‚úÖ' if ok else '‚ùå'} {query[:45]:45} -> {tools[:2] if tools else 'NO TOOL'}\")\n",
    "\n",
    "print(f\"\\nüìä Traditional: {trad_correct}/{len(TESTS)} ({100*trad_correct/len(TESTS):.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Test 2: Semantic Approach (Pre-filtered Tools)\n",
    "\n",
    "This test uses `registry.py` to dynamically select which tools the agent receives. Here's how it works:\n",
    "\n",
    "### How the Tool Registry Works (`registry.py`)\n",
    "\n",
    "**1. Building the vector index ‚Äî `build_index(tools)`**\n",
    "\n",
    "Called once at startup. For each tool, it concatenates `name: docstring` (e.g., `\"get_hotel_pricing: Get room pricing for a hotel.\"`) and encodes it into a 384-dimensional vector using `SentenceTransformer('all-MiniLM-L6-v2')`. All vectors are added to a FAISS `IndexFlatL2` index (L2 = Euclidean distance). The index lives in memory ‚Äî if it doesn't exist yet, `search_tools()` will fail, so `build_index()` must run first.\n",
    "\n",
    "**2. Querying ‚Äî `search_tools(query, top_k=3)`**\n",
    "\n",
    "When a user query arrives, it's embedded with the same model and searched against the FAISS index. FAISS returns the `top_k` nearest tool vectors by distance. The function returns the actual tool callables (not just names), ready to pass directly to `Agent(tools=...)`.\n",
    "\n",
    "**3. Agent creation ‚Äî per-query tool injection**\n",
    "\n",
    "For each query, a new `Agent` is created with **only the top-3 tools**. The other 28 tools are never passed to the agent ‚Äî they're not removed, they simply never enter the agent's context. This is the key difference: the agent's tool selection space shrinks from 31 to 3.\n",
    "\n",
    "**4. Why `all-MiniLM-L6-v2`?**\n",
    "\n",
    "It's a lightweight sentence embedding model (22M params, 384 dims) optimized for semantic similarity. It runs fast on CPU, which matters here since we're encoding short strings (tool descriptions), not documents. Larger models would be overkill for matching a query like \"How much for a room?\" to `\"get_hotel_pricing: Get room pricing for a hotel.\"`\n",
    "\n",
    "```\n",
    "build_index(ALL_TOOLS)          # Once: encode 31 tool descriptions ‚Üí FAISS\n",
    "selected = search_tools(query)  # Per query: embed query ‚Üí top-3 tools\n",
    "Agent(tools=selected)           # Agent only sees 3 tools, not 31\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"TEST 2: SEMANTIC - 3 tools per query\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "sem_results = []\n",
    "sem_correct = 0\n",
    "\n",
    "for query, expected in TESTS:\n",
    "    selected = search_tools(query, top_k=3)\n",
    "    selected_names = [t.__name__ for t in selected]\n",
    "    \n",
    "    agent = Agent(tools=selected, system_prompt=PROMPT, model=MODEL)\n",
    "    tools = run_and_capture(agent, query)\n",
    "    ok = expected in tools\n",
    "    sem_correct += ok\n",
    "    sem_results.append({'query': query, 'expected': expected, 'selected': selected_names, 'actual': tools, 'correct': ok})\n",
    "    print(f\"{'‚úÖ' if ok else '‚ùå'} {query[:45]:45} -> {tools[:2] if tools else 'NO TOOL'}\")\n",
    "    print(f\"   Available: {selected_names}\")\n",
    "\n",
    "print(f\"\\nüìä Semantic: {sem_correct}/{len(TESTS)} ({100*sem_correct/len(TESTS):.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Test 3: Semantic Approach with Memory (Single Agent)\n",
    "\n",
    "Tests 1 and 2 create a **new agent per query**. That works for benchmarking, but in production you lose conversation history ‚Äî the agent can't reference previous answers or maintain context across turns.\n",
    "\n",
    "### The Problem\n",
    "\n",
    "Strands agents store conversation history in `agent.messages`. When you do `Agent(tools=selected)` for each query, you get a fresh agent with an empty message list. Multi-turn conversations break.\n",
    "\n",
    "### The Solution: `swap_tools(agent, new_tools)`\n",
    "\n",
    "Instead of recreating the agent, we manipulate its `tool_registry` directly:\n",
    "\n",
    "1. Clear `agent.tool_registry.registry` and `agent.tool_registry.dynamic_tools`\n",
    "2. Re-register only the tools returned by `search_tools(query)`\n",
    "3. Call the same agent ‚Äî `agent.messages` is untouched\n",
    "\n",
    "This works because Strands calls `tool_registry.get_all_tools_config()` at each event loop cycle, so runtime changes to the registry are picked up on the next `agent()` call.\n",
    "\n",
    "```\n",
    "agent = Agent(tools=initial_tools)   # Create once\n",
    "for query in queries:\n",
    "    selected = search_tools(query)    # FAISS top-3\n",
    "    swap_tools(agent, selected)       # Swap registry, keep memory\n",
    "    agent(query)                      # Conversation history preserved\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from registry import swap_tools\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"TEST 3: SEMANTIC + MEMORY - 3 tools per query, single agent\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create ONE agent with initial tools (first query's tools)\n",
    "initial_tools = search_tools(TESTS[0][0], top_k=3)\n",
    "memory_agent = Agent(tools=initial_tools, system_prompt=PROMPT, model=MODEL)\n",
    "\n",
    "mem_results = []\n",
    "mem_correct = 0\n",
    "\n",
    "for i, (query, expected) in enumerate(TESTS):\n",
    "    # Swap tools for this query (memory preserved)\n",
    "    selected = search_tools(query, top_k=3)\n",
    "    selected_names = [t.__name__ for t in selected]\n",
    "    swap_tools(memory_agent, selected)\n",
    "    \n",
    "    tools = run_and_capture(memory_agent, query)\n",
    "    ok = expected in tools\n",
    "    mem_correct += ok\n",
    "    mem_results.append({'query': query, 'expected': expected, 'selected': selected_names, 'actual': tools, 'correct': ok})\n",
    "    print(f\"{'‚úÖ' if ok else '‚ùå'} {query[:45]:45} -> {tools[:2] if tools else 'NO TOOL'}\")\n",
    "    print(f\"   Available: {selected_names} | Memory: {len(memory_agent.messages)} msgs\")\n",
    "\n",
    "print(f\"\\nüìä Semantic+Memory: {mem_correct}/{len(TESTS)} ({100*mem_correct/len(TESTS):.1f}%)\")\n",
    "print(f\"üí¨ Total conversation messages: {len(memory_agent.messages)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Ground Truth Verification\n",
    "\n",
    "We compare each agent's tool selection against the known correct tool. Any mismatch is a **tool selection hallucination**. For semantic errors, we also check whether the correct tool was even in the top-3 ‚Äî if it wasn't, the error is in the embedding search, not the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"GROUND TRUTH VERIFICATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nüìä Results:\")\n",
    "print(f\"   Traditional:      {trad_correct}/{len(TESTS)} ({100*trad_correct/len(TESTS):.1f}%)\")\n",
    "print(f\"   Semantic:         {sem_correct}/{len(TESTS)} ({100*sem_correct/len(TESTS):.1f}%)\")\n",
    "print(f\"   Semantic+Memory:  {mem_correct}/{len(TESTS)} ({100*mem_correct/len(TESTS):.1f}%)\")\n",
    "print(f\"   Improvement:      +{sem_correct - trad_correct} queries (+{100*(sem_correct-trad_correct)/len(TESTS):.1f}%)\")\n",
    "\n",
    "print(f\"\\nüîç Traditional Errors:\")\n",
    "for r in trad_results:\n",
    "    if not r['correct']:\n",
    "        print(f\"   ‚ùå '{r['query'][:50]}'\")\n",
    "        print(f\"      Expected: {r['expected']}, Got: {r['actual']}\")\n",
    "\n",
    "print(f\"\\nüîç Semantic Errors:\")\n",
    "for r in sem_results:\n",
    "    if not r['correct']:\n",
    "        print(f\"   ‚ùå '{r['query'][:50]}'\")\n",
    "        print(f\"      Expected: {r['expected']}, Got: {r['actual']}\")\n",
    "        print(f\"      Available: {r['selected']}\")\n",
    "        if r['expected'] not in r['selected']:\n",
    "            print(f\"      ‚ö†Ô∏è  Correct tool NOT in top-3!\")\n",
    "\n",
    "print(f\"\\nüîç Semantic+Memory Errors:\")\n",
    "for r in mem_results:\n",
    "    if not r['correct']:\n",
    "        print(f\"   ‚ùå '{r['query'][:50]}'\")\n",
    "        print(f\"      Expected: {r['expected']}, Got: {r['actual']}\")\n",
    "        print(f\"      Available: {r['selected']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "| Approach | Tools Available | Memory | Accuracy | Hallucination Rate |\n",
    "|----------|----------------|--------|----------|--------------------|\n",
    "| Traditional | 33 tools | ‚ùå New agent/query | X/13 (Y%) | (100-Y)% |\n",
    "| Semantic | 3 tools/query | ‚ùå New agent/query | X/13 (Y%) | (100-Y)% |\n",
    "| Semantic+Memory | 3 tools/query | ‚úÖ Single agent | X/13 (Y%) | (100-Y)% |\n",
    "\n",
    "### Why Semantic Tool Discovery Reduces Hallucinations\n",
    "\n",
    "1. **Reduced Search Space**: 3 tools instead of 33 ‚Üí 91% reduction\n",
    "2. **Semantic Relevance**: Pre-filtered by meaning\n",
    "3. **Clearer Context**: Better understanding of each tool\n",
    "4. **Less Confusion**: Similar tools are separated\n",
    "5. **Memory Compatible**: `swap_tools()` preserves conversation history while still pre-filtering\n",
    "\n",
    "### Limitations\n",
    "\n",
    "- Semantic search quality depends on docstring quality\n",
    "- If correct tool not in top-3, agent will fail\n",
    "- Requires upfront embedding computation\n",
    "\n",
    "### References\n",
    "\n",
    "- [Internal Representations as Indicators of Hallucinations in Agent Tool Selection](https://arxiv.org/pdf/2601.05214)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
