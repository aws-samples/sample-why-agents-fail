{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Tool Selection: Token Efficiency & Accuracy Analysis\n",
    "\n",
    "Based on: [Internal Representations as Indicators of Hallucinations in Agent Tool Selection](https://arxiv.org/pdf/2601.05214)\n",
    "\n",
    "## What This Demo Measures\n",
    "\n",
    "This notebook compares **Traditional** (all 31 tools) vs **Semantic** (top-3 filtered tools) approaches across two critical metrics:\n",
    "\n",
    "1. **Token Consumption**: How many tokens are used per query?\n",
    "2. **Tool Selection Accuracy**: Does the agent pick the correct tool?\n",
    "\n",
    "### The Dual Problem\n",
    "\n",
    "- âŒ **Token Waste**: Sending 31 tool descriptions = ~4,500 tokens per query\n",
    "- âŒ **Hallucination Risk**: More tools = more confusion = wrong tool selection\n",
    "\n",
    "### The Solution\n",
    "\n",
    "```\n",
    "User Query â†’ FAISS Search â†’ Top 3 Tools â†’ Agent â†’ Correct Selection + Fewer Tokens\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Uncomment to set manually:\n",
    "# os.environ['OPENAI_API_KEY'] = 'your-key-here'\n",
    "\n",
    "assert os.getenv('OPENAI_API_KEY'), 'Set OPENAI_API_KEY in .env file or uncomment line above'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import io\n",
    "import re\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "from strands import Agent\n",
    "from strands.models.openai import OpenAIModel\n",
    "from enhanced_tools import ALL_TOOLS\n",
    "from registry import build_index, search_tools, swap_tools\n",
    "\n",
    "print(f\"âœ… Loaded {len(ALL_TOOLS)} tools\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Semantic Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_index(ALL_TOOLS)\n",
    "print(\"âœ… FAISS index built\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Queries\n",
    "\n",
    "Diverse queries testing different tool categories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TESTS = [\n",
    "    # Ambiguous queries - could match multiple tools\n",
    "    (\"Search for something in Paris\", \"search\"),  # Generic vs search_hotels\n",
    "    (\"Check something\", \"check\"),  # Generic vs check_hotel_availability\n",
    "    (\"Get some details\", \"get_details\"),  # Generic vs get_hotel_details\n",
    "    (\"What's the status?\", \"get_status\"),  # Generic vs get_flight_status\n",
    "    (\"I need information\", \"get_info\"),  # Generic vs specific tools\n",
    "    \n",
    "    # Hotel queries - similar tools\n",
    "    (\"Search hotels in Barcelona\", \"search_hotels\"),  # vs search_real_hotels\n",
    "    (\"Find real hotels in France\", \"search_real_hotels\"),  # vs search_hotels\n",
    "    (\"How much does a hotel cost?\", \"get_hotel_pricing\"),  # vs get_hotel_details\n",
    "    (\"What amenities does the hotel have?\", \"get_hotel_details\"),  # vs get_hotel_pricing\n",
    "    (\"Is the hotel available tomorrow?\", \"check_hotel_availability\"),  # vs check_hotel_availability_dates\n",
    "    (\"Check availability from March 15 to 18\", \"check_hotel_availability_dates\"),  # vs check_hotel_availability\n",
    "    \n",
    "    # Flight queries - similar tools\n",
    "    (\"Find flights to Tokyo\", \"search_flights\"),  # vs search_flight_prices\n",
    "    (\"How much do flights cost?\", \"search_flight_prices\"),  # vs search_flights\n",
    "    (\"Is flight AA123 on time?\", \"get_flight_status\"),  # vs get_flight_details\n",
    "    \n",
    "    # Booking queries - generic vs specific\n",
    "    (\"Book a hotel for me\", \"book_hotel\"),  # vs book (generic)\n",
    "    (\"Book a flight for me\", \"book_flight\"),  # vs book (generic)\n",
    "    (\"Book something\", \"book\"),  # Generic\n",
    "    \n",
    "    # Clear specific queries\n",
    "    (\"Convert 500 USD to EUR\", \"get_currency_exchange\"),\n",
    "    (\"Do I need a visa for Spain?\", \"get_travel_documents\"),\n",
    "]\n",
    "\n",
    "print(f\"ðŸ“‹ Test suite: {len(TESTS)} queries (ambiguous queries to test tool confusion)\")\n",
    "print(f\"ðŸ“Š Tool pool: {len(ALL_TOOLS)} tools\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_and_capture_with_tokens(agent, query: str) -> Tuple[List[str], Dict]:\n",
    "    \"\"\"Run agent and capture tool calls + token usage\"\"\"\n",
    "    old_stdout = sys.stdout\n",
    "    sys.stdout = io.StringIO()\n",
    "    \n",
    "    try:\n",
    "        result = agent(query)\n",
    "        output = sys.stdout.getvalue()\n",
    "    finally:\n",
    "        sys.stdout = old_stdout\n",
    "    \n",
    "    # Extract tool calls\n",
    "    tools = re.findall(r'Tool #\\d+: (\\w+)', output)\n",
    "    \n",
    "    # Extract token usage from result\n",
    "    tokens = {'input': 0, 'output': 0, 'total': 0}\n",
    "    \n",
    "    if hasattr(result, 'metrics'):\n",
    "        summary = result.metrics.get_summary()\n",
    "        usage = summary.get('accumulated_usage', {})\n",
    "        tokens['input'] = usage.get('inputTokens', 0)\n",
    "        tokens['output'] = usage.get('outputTokens', 0)\n",
    "        tokens['total'] = usage.get('totalTokens', 0)\n",
    "    \n",
    "    # Fallback: estimate if no usage data\n",
    "    if tokens['total'] == 0:\n",
    "        num_tools = len(agent.tool_registry.get_all_tools_config())\n",
    "        tokens['input'] = num_tools * 50 + 100  # ~50 tokens per tool + prompt\n",
    "        tokens['output'] = 50  # estimated\n",
    "        tokens['total'] = tokens['input'] + tokens['output']\n",
    "        tokens['estimated'] = True\n",
    "    \n",
    "    return tools, tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 1: Traditional Approach (All 29 Tools)\n",
    "\n",
    "Agent receives all tools on every query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = OpenAIModel(model_id=\"gpt-4o-mini\")\n",
    "PROMPT = \"You are a travel assistant. Use the correct tool to answer questions.\"\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(f\"TEST 1: TRADITIONAL - {len(ALL_TOOLS)} tools every query\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "trad_results = []\n",
    "trad_correct = 0\n",
    "trad_total_tokens = 0\n",
    "\n",
    "for query, expected in TESTS:\n",
    "    agent = Agent(tools=ALL_TOOLS, system_prompt=PROMPT, model=MODEL)\n",
    "    tools, tokens = run_and_capture_with_tokens(agent, query)\n",
    "    \n",
    "    ok = expected in tools\n",
    "    trad_correct += ok\n",
    "    trad_total_tokens += tokens['total']\n",
    "    \n",
    "    trad_results.append({\n",
    "        'query': query,\n",
    "        'expected': expected,\n",
    "        'actual': tools,\n",
    "        'correct': ok,\n",
    "        'tokens': tokens\n",
    "    })\n",
    "    \n",
    "    status = 'âœ…' if ok else 'âŒ'\n",
    "    est = ' (est)' if tokens.get('estimated') else ''\n",
    "    print(f\"{status} {query[:50]:50} | {tokens['total']:5} tokens{est}\")\n",
    "    print(f\"   Called: {tools[:2] if tools else 'NO TOOL'}\")\n",
    "\n",
    "print(f\"\\nðŸ“Š Traditional Results:\")\n",
    "print(f\"   Accuracy: {trad_correct}/{len(TESTS)} ({100*trad_correct/len(TESTS):.1f}%)\")\n",
    "print(f\"   Total tokens: {trad_total_tokens:,}\")\n",
    "print(f\"   Avg tokens/query: {trad_total_tokens/len(TESTS):.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 2: Semantic Approach (Top-3 Filtered Tools)\n",
    "\n",
    "Agent receives only the 3 most relevant tools per query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"TEST 2: SEMANTIC - Top-3 tools per query\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "sem_results = []\n",
    "sem_correct = 0\n",
    "sem_total_tokens = 0\n",
    "\n",
    "for query, expected in TESTS:\n",
    "    selected = search_tools(query, top_k=3)\n",
    "    selected_names = [t.__name__ for t in selected]\n",
    "    \n",
    "    agent = Agent(tools=selected, system_prompt=PROMPT, model=MODEL)\n",
    "    tools, tokens = run_and_capture_with_tokens(agent, query)\n",
    "    \n",
    "    ok = expected in tools\n",
    "    sem_correct += ok\n",
    "    sem_total_tokens += tokens['total']\n",
    "    \n",
    "    sem_results.append({\n",
    "        'query': query,\n",
    "        'expected': expected,\n",
    "        'selected': selected_names,\n",
    "        'actual': tools,\n",
    "        'correct': ok,\n",
    "        'tokens': tokens\n",
    "    })\n",
    "    \n",
    "    status = 'âœ…' if ok else 'âŒ'\n",
    "    est = ' (est)' if tokens.get('estimated') else ''\n",
    "    print(f\"{status} {query[:50]:50} | {tokens['total']:5} tokens{est}\")\n",
    "    print(f\"   Available: {selected_names}\")\n",
    "    print(f\"   Called: {tools[:2] if tools else 'NO TOOL'}\")\n",
    "\n",
    "print(f\"\\nðŸ“Š Semantic Results:\")\n",
    "print(f\"   Accuracy: {sem_correct}/{len(TESTS)} ({100*sem_correct/len(TESTS):.1f}%)\")\n",
    "print(f\"   Total tokens: {sem_total_tokens:,}\")\n",
    "print(f\"   Avg tokens/query: {sem_total_tokens/len(TESTS):.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 3: Semantic + Memory (Single Agent)\n",
    "\n",
    "Same agent across all queries, tools swapped dynamically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"TEST 3: SEMANTIC + MEMORY - Single agent, dynamic tool swapping\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "initial_tools = search_tools(TESTS[0][0], top_k=3)\n",
    "memory_agent = Agent(tools=initial_tools, system_prompt=PROMPT, model=MODEL)\n",
    "\n",
    "mem_results = []\n",
    "mem_correct = 0\n",
    "mem_total_tokens = 0\n",
    "\n",
    "for query, expected in TESTS:\n",
    "    selected = search_tools(query, top_k=3)\n",
    "    selected_names = [t.__name__ for t in selected]\n",
    "    swap_tools(memory_agent, selected)\n",
    "    \n",
    "    tools, tokens = run_and_capture_with_tokens(memory_agent, query)\n",
    "    \n",
    "    ok = expected in tools\n",
    "    mem_correct += ok\n",
    "    mem_total_tokens += tokens['total']\n",
    "    \n",
    "    mem_results.append({\n",
    "        'query': query,\n",
    "        'expected': expected,\n",
    "        'selected': selected_names,\n",
    "        'actual': tools,\n",
    "        'correct': ok,\n",
    "        'tokens': tokens,\n",
    "        'messages': len(memory_agent.messages)\n",
    "    })\n",
    "    \n",
    "    status = 'âœ…' if ok else 'âŒ'\n",
    "    est = ' (est)' if tokens.get('estimated') else ''\n",
    "    print(f\"{status} {query[:50]:50} | {tokens['total']:5} tokens{est}\")\n",
    "    print(f\"   Available: {selected_names} | Memory: {len(memory_agent.messages)} msgs\")\n",
    "    print(f\"   Called: {tools[:2] if tools else 'NO TOOL'}\")\n",
    "\n",
    "print(f\"\\nðŸ“Š Semantic+Memory Results:\")\n",
    "print(f\"   Accuracy: {mem_correct}/{len(TESTS)} ({100*mem_correct/len(TESTS):.1f}%)\")\n",
    "print(f\"   Total tokens: {mem_total_tokens:,}\")\n",
    "print(f\"   Avg tokens/query: {mem_total_tokens/len(TESTS):.0f}\")\n",
    "print(f\"   Final conversation: {len(memory_agent.messages)} messages\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparative Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"COMPARATIVE ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nðŸ“Š Accuracy Comparison:\")\n",
    "print(f\"   Traditional:      {trad_correct}/{len(TESTS)} ({100*trad_correct/len(TESTS):.1f}%)\")\n",
    "print(f\"   Semantic:         {sem_correct}/{len(TESTS)} ({100*sem_correct/len(TESTS):.1f}%)\")\n",
    "print(f\"   Semantic+Memory:  {mem_correct}/{len(TESTS)} ({100*mem_correct/len(TESTS):.1f}%)\")\n",
    "\n",
    "if sem_correct > trad_correct:\n",
    "    print(f\"   âœ… Semantic improved by +{sem_correct - trad_correct} queries\")\n",
    "elif sem_correct < trad_correct:\n",
    "    print(f\"   âš ï¸  Semantic decreased by {trad_correct - sem_correct} queries\")\n",
    "else:\n",
    "    print(f\"   âž¡ï¸  Same accuracy\")\n",
    "\n",
    "print(f\"\\nðŸ’° Token Consumption:\")\n",
    "print(f\"   Traditional:      {trad_total_tokens:,} tokens ({trad_total_tokens/len(TESTS):.0f} avg)\")\n",
    "print(f\"   Semantic:         {sem_total_tokens:,} tokens ({sem_total_tokens/len(TESTS):.0f} avg)\")\n",
    "print(f\"   Semantic+Memory:  {mem_total_tokens:,} tokens ({mem_total_tokens/len(TESTS):.0f} avg)\")\n",
    "\n",
    "if trad_total_tokens > 0:\n",
    "    sem_savings = trad_total_tokens - sem_total_tokens\n",
    "    mem_savings = trad_total_tokens - mem_total_tokens\n",
    "    print(f\"\\nðŸ’¡ Token Savings:\")\n",
    "    print(f\"   Semantic vs Traditional:  {sem_savings:,} tokens ({100*sem_savings/trad_total_tokens:.1f}% reduction)\")\n",
    "    print(f\"   Memory vs Traditional:    {mem_savings:,} tokens ({100*mem_savings/trad_total_tokens:.1f}% reduction)\")\n",
    "    \n",
    "    if mem_total_tokens > sem_total_tokens:\n",
    "        overhead = mem_total_tokens - sem_total_tokens\n",
    "        print(f\"   Memory overhead:          +{overhead:,} tokens (conversation history)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Per-Query Token Breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"PER-QUERY TOKEN BREAKDOWN\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n{'Query':<50} {'Trad':>8} {'Sem':>8} {'Mem':>8} {'Saved':>8}\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "for i in range(len(TESTS)):\n",
    "    query = TESTS[i][0][:49]\n",
    "    trad_tok = trad_results[i]['tokens']['total']\n",
    "    sem_tok = sem_results[i]['tokens']['total']\n",
    "    mem_tok = mem_results[i]['tokens']['total']\n",
    "    saved = trad_tok - sem_tok\n",
    "    \n",
    "    print(f\"{query:<50} {trad_tok:8} {sem_tok:8} {mem_tok:8} {saved:8}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"ERROR ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nðŸ” Traditional Errors:\")\n",
    "trad_errors = [r for r in trad_results if not r['correct']]\n",
    "if trad_errors:\n",
    "    for r in trad_errors:\n",
    "        print(f\"   âŒ '{r['query'][:60]}'\")\n",
    "        print(f\"      Expected: {r['expected']}, Got: {r['actual']}\")\n",
    "else:\n",
    "    print(\"   âœ… No errors\")\n",
    "\n",
    "print(f\"\\nðŸ” Semantic Errors:\")\n",
    "sem_errors = [r for r in sem_results if not r['correct']]\n",
    "if sem_errors:\n",
    "    for r in sem_errors:\n",
    "        print(f\"   âŒ '{r['query'][:60]}'\")\n",
    "        print(f\"      Expected: {r['expected']}, Got: {r['actual']}\")\n",
    "        print(f\"      Available: {r['selected']}\")\n",
    "        if r['expected'] not in r['selected']:\n",
    "            print(f\"      âš ï¸  Correct tool NOT in top-3 (FAISS filtering issue)\")\n",
    "else:\n",
    "    print(\"   âœ… No errors\")\n",
    "\n",
    "print(f\"\\nðŸ” Semantic+Memory Errors:\")\n",
    "mem_errors = [r for r in mem_results if not r['correct']]\n",
    "if mem_errors:\n",
    "    for r in mem_errors:\n",
    "        print(f\"   âŒ '{r['query'][:60]}'\")\n",
    "        print(f\"      Expected: {r['expected']}, Got: {r['actual']}\")\n",
    "        print(f\"      Available: {r['selected']}\")\n",
    "else:\n",
    "    print(\"   âœ… No errors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "| Approach | Tools/Query | Accuracy | Avg Tokens | Token Savings |\n",
    "|----------|-------------|----------|------------|---------------|\n",
    "| Traditional | 31 tools | X% | ~Y tokens | Baseline |\n",
    "| Semantic | 3 tools | X% | ~Y tokens | Z% |\n",
    "| Semantic+Memory | 3 tools | X% | ~Y tokens | Z% |\n",
    "\n",
    "### Why Semantic Tool Selection Works\n",
    "\n",
    "1. **Reduced Token Waste**: 3 tools vs 31 = ~89% fewer tokens on tool descriptions\n",
    "2. **Better Accuracy**: Fewer choices = less confusion = correct tool selection\n",
    "3. **Production Ready**: `swap_tools()` preserves conversation memory\n",
    "\n",
    "### References\n",
    "\n",
    "- [Internal Representations as Indicators of Hallucinations in Agent Tool Selection](https://arxiv.org/pdf/2601.05214)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
